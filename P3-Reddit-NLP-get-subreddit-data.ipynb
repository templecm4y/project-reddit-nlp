{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit API Data Scraping\n",
    "---\n",
    "\n",
    "In this notebook, I utilize Reddit's built in API .json functionality to scrape post data from four subreddits. I then export this data into .csv files to use in my analysis notebook. \n",
    "\n",
    "My chosen subreddits are as follows:\n",
    "\n",
    "- r/nba\n",
    "- r/nfl\n",
    "- r/cfb\n",
    "- r/CollegeBasketball\n",
    "\n",
    "I have taken mostly new posts from the subreddits, but I have also supplemented this with the top 500 posts from the past year into each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update pandas global settings to view all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import existing subreddit data\n",
    "nba_df = pd.read_csv('./data/nba_subreddit_data.csv')\n",
    "nfl_df = pd.read_csv('./data/nfl_subreddit_data.csv')\n",
    "cfb_df = pd.read_csv('./data/cfb_subreddit_data.csv')\n",
    "cbb_df = pd.read_csv('./data/cbb_subreddit_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2507, 100)\n",
      "(2976, 100)\n",
      "(1984, 103)\n",
      "(1995, 104)\n"
     ]
    }
   ],
   "source": [
    "# check shape of dataframes\n",
    "print(nba_df.shape)\n",
    "print(nfl_df.shape)\n",
    "print(cfb_df.shape)\n",
    "print(cbb_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter subreddit urls\n",
    "nba_url = 'https://www.reddit.com/r/nba.json'\n",
    "nfl_url = 'https://www.reddit.com/r/nfl.json'\n",
    "cfb_url = 'https://www.reddit.com/r/cfb.json'\n",
    "cbb_url = 'https://www.reddit.com/r/collegebasketball.json'\n",
    "# establish our header\n",
    "header = {'User-agent': 'subreddit get requests'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial get request to test API\n",
    "res = requests.get(cfb_url, headers=header)\n",
    "cfb_res = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check request status\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['approved_at_utc', 'subreddit', 'selftext', 'author_fullname', 'saved', 'mod_reason_title', 'gilded', 'clicked', 'title', 'link_flair_richtext', 'subreddit_name_prefixed', 'hidden', 'pwls', 'link_flair_css_class', 'downs', 'thumbnail_height', 'hide_score', 'name', 'quarantine', 'link_flair_text_color', 'author_flair_background_color', 'subreddit_type', 'ups', 'total_awards_received', 'media_embed', 'thumbnail_width', 'author_flair_template_id', 'is_original_content', 'user_reports', 'secure_media', 'is_reddit_media_domain', 'is_meta', 'category', 'secure_media_embed', 'link_flair_text', 'can_mod_post', 'score', 'approved_by', 'thumbnail', 'edited', 'author_flair_css_class', 'author_flair_richtext', 'gildings', 'content_categories', 'is_self', 'mod_note', 'created', 'link_flair_type', 'wls', 'banned_by', 'author_flair_type', 'domain', 'allow_live_comments', 'selftext_html', 'likes', 'suggested_sort', 'banned_at_utc', 'view_count', 'archived', 'no_follow', 'is_crosspostable', 'pinned', 'over_18', 'all_awardings', 'media_only', 'can_gild', 'spoiler', 'locked', 'author_flair_text', 'visited', 'num_reports', 'distinguished', 'subreddit_id', 'mod_reason_by', 'removal_reason', 'link_flair_background_color', 'id', 'is_robot_indexable', 'report_reasons', 'author', 'num_crossposts', 'num_comments', 'send_replies', 'whitelist_status', 'contest_mode', 'mod_reports', 'author_patreon_flair', 'author_flair_text_color', 'permalink', 'parent_whitelist_status', 'stickied', 'url', 'subreddit_subscribers', 'created_utc', 'discussion_type', 'media', 'is_video'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore keys for test request\n",
    "cfb_res['data']['children'][0]['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define function to get num pages of posts from a subreddit, start collecting at a defined after\n",
    "def reddit_scraper(url, num, after = None):\n",
    "    posts = []\n",
    "    # loop through the num pages, each subreddit .json returns 25 posts \n",
    "    for page in range(num):\n",
    "        # initiate params modifier for posts if there no defined after\n",
    "        if after == None:\n",
    "            params = {}\n",
    "        # add in after id for each loop following to ensure no duplicate posts\n",
    "        else:\n",
    "            params = {'after': after}\n",
    "        # call our get request for the posts\n",
    "        res = requests.get(url, params=params, headers=header)\n",
    "        # check status code, 200 means posts were successfully downloaded\n",
    "        if res.status_code == 200:\n",
    "            # convert request to .json\n",
    "            new_json = res.json()\n",
    "            # extend list from the 'children' dictionary for each request\n",
    "            posts.extend(new_json['data']['children'])\n",
    "            # update after id\n",
    "            after = new_json['data']['after']\n",
    "        else:\n",
    "            # print status code if not 200\n",
    "            print(res.status_code)\n",
    "            break\n",
    "        # wait 1 second\n",
    "        time.sleep(1)\n",
    "        \n",
    "    # create a new dataframe with the 'data' from each post\n",
    "    new_df = pd.DataFrame([post['data'] for post in posts])\n",
    "    \n",
    "    # print final value of after\n",
    "    print(f'Final value of after parameter: {after}')\n",
    "    \n",
    "    # return the dataframe\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data from r/nba\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final value of after parameter: t3_cc3vwn\n"
     ]
    }
   ],
   "source": [
    "# call subreddit scraping function\n",
    "new_nba_df = reddit_scraper(nba_url, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 98)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape of scraped dataframe\n",
    "new_nba_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_nba_df = pd.concat([nba_df, new_nba_df], axis=0, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2759, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm concatenation\n",
    "new_nba_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "new_nba_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2062"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of unique posts\n",
    "new_nba_df['name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CSV of original and new data\n",
    "new_nba_df.to_csv(\"./data/nba_subreddit_data.csv\", index=False)\n",
    "nba_df.to_csv(\"./data/nba_subreddit_data - backup.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data from r/nfl\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final value of after parameter: None\n"
     ]
    }
   ],
   "source": [
    "# call subreddit scraping function\n",
    "new_nfl_df = reddit_scraper(nfl_url, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(995, 99)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape of scraped dataframe\n",
    "new_nfl_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine dataframes and reset index\n",
    "new_nfl_df = pd.concat([nfl_df, new_nfl_df], axis=0, sort=True)\n",
    "new_nfl_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2976, 100)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm concatenation\n",
    "new_nfl_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1787"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of unique posts\n",
    "new_nfl_df['name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CSV of original and new data\n",
    "new_nfl_df.to_csv(\"./data/nfl_subreddit_data.csv\", index=False)\n",
    "nfl_df.to_csv(\"./data/nfl_subreddit_data - backup.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data from r/cfb\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final value of after parameter: t3_capdm9\n"
     ]
    }
   ],
   "source": [
    "# call subreddit scraping function\n",
    "new_cfb_df = reddit_scraper(cfb_url, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(984, 103)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape of scraped dataframe\n",
    "new_cfb_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine dataframes and reset index\n",
    "new_cfb_df = pd.concat([cfb_df, new_cfb_df], axis=0, sort=True)\n",
    "new_cfb_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1984, 103)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm concatenation\n",
    "new_cfb_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1861"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of unique posts\n",
    "new_cfb_df['name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CSV of original and new data\n",
    "new_cfb_df.to_csv(\"./data/cfb_subreddit_data.csv\", index=False)\n",
    "cfb_df.to_csv(\"./data/cfb_subreddit_data - backup.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data from r/CollegeBasketball\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final value of after parameter: t3_car435\n"
     ]
    }
   ],
   "source": [
    "# call subreddit scraping function\n",
    "new_cbb_df = reddit_scraper(cbb_url, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(995, 104)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape of scraped dataframe\n",
    "new_cbb_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine dataframes and reset index\n",
    "new_cbb_df = pd.concat([cbb_df, new_cbb_df], axis=0, sort=True)\n",
    "new_cbb_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1995, 104)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm concatenation\n",
    "new_cbb_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1885"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of unique posts\n",
    "new_cbb_df['name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CSV of original and new data\n",
    "new_cbb_df.to_csv(\"./data/cbb_subreddit_data.csv\", index=False)\n",
    "cbb_df.to_csv(\"./data/cbb_subreddit_data - backup.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
